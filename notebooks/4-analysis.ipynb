{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import prody as pd\n",
    "import pynndescent\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scripts import plotting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paper_dir = Path(\"data\")\n",
    "paper_dir.mkdir(exist_ok=True)\n",
    "protein_dir = Path(\"data/proteins\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uniref_s_file = paper_dir / \"uniref50_10_70_95_shapemers.txt\"\n",
    "uniref_i_file = paper_dir / \"uniref50_10_70_95_indices.txt\"\n",
    "uniref_wv_file = paper_dir / \"uniref50_10_70_95_word2vec.txt\"\n",
    "uniref_topic_file = paper_dir / \"uniref50_casp12_nmf_400.pkl\"\n",
    "\n",
    "pdb_s_file = paper_dir / \"pdb_chain_10_shapemers.txt\"\n",
    "pdb_i_file = paper_dir / \"pdb_chain_10_indices.txt\"\n",
    "pdb_wv_file = paper_dir / \"pdb_chain_10_word2vec.txt\"\n",
    "pdb_topic_file = paper_dir / \"pdb_casp12_nmf_400.pkl\"\n",
    "\n",
    "swissprot_s_file = paper_dir / \"swissprot_10_shapemers.txt\"\n",
    "swissprot_i_file = paper_dir / \"swissprot_10_indices.txt\"\n",
    "swissprot_wv_file = paper_dir / \"swissprot_10_word2vec.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(paper_dir / \"pdb_chain_word2vec_isolation_forest.pkl\", \"rb\") as f:\n",
    "    forest = pickle.load(f)\n",
    "\n",
    "with open(paper_dir / \"casp12_nmf_400_model.pkl\", \"rb\") as f:\n",
    "    vectorizer, topic_model = pickle.load(f)\n",
    "\n",
    "word2vec = Word2Vec.load(str(paper_dir / \"pdb_chain_word2vec_1024.model\"), mmap='r')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(paper_dir / \"pdb_uniref50_word2vec_embeddings_scores.pkl\", \"rb\") as f:\n",
    "    wv_keys, wv_embeddings, wv_scores = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uniref50_id_to_darkness = {}\n",
    "uniref50_id_to_num = {}\n",
    "uniref50_id_to_cluster = {}\n",
    "with open(\"data/AFDBv3_UniRef50.csv\") as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        parts = line.strip().split(\",\")\n",
    "        key = f\"{parts[-1]}-F1\"\n",
    "        if key in wv_keys:\n",
    "            uniref50_id_to_darkness[key] = max(0., float(parts[5]))\n",
    "            uniref50_id_to_num[key] = int(parts[-4])\n",
    "            uniref50_id_to_cluster[key] = parts[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Repeat proteins"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "repeat_protein_counts = defaultdict(set)\n",
    "with open(uniref_s_file) as f:\n",
    "    for line in tqdm(f):\n",
    "        key, shapemers = line.strip().split(\"\\t\")\n",
    "        shapemers = list(map(int, shapemers.split()))\n",
    "        most_common_shapemer, most_common_count = Counter(shapemers).most_common(1)[0]\n",
    "        fraction = most_common_count / len(shapemers)\n",
    "        if fraction > 0.5:\n",
    "            repeat_protein_counts[most_common_shapemer].add(key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SIFTS comparison"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "swissprot_lengths = {}\n",
    "with open(swissprot_i_file) as f:\n",
    "    for line in tqdm(f):\n",
    "        key, indices = line.strip().split(\"\\t\")\n",
    "        swissprot_lengths[key] = max(map(int, indices.split())) + 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdb_lengths = {}\n",
    "with open(pdb_s_file) as f:\n",
    "    for line in tqdm(f):\n",
    "        key, shapemers = line.strip().split(\"\\t\")\n",
    "        pdb_lengths[key] = len(shapemers.split()) + 16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdb_keys = []\n",
    "pdb_embeddings = []\n",
    "swissprot_keys = []\n",
    "swissprot_embeddings = []\n",
    "\n",
    "with open(pdb_wv_file) as f:\n",
    "    for line in tqdm(f):\n",
    "        key, vector, score = line.strip().split(\"\\t\")\n",
    "        pdb_keys.append(key)\n",
    "        pdb_embeddings.append(list(map(float, vector.split())))\n",
    "\n",
    "with open(swissprot_wv_file) as f:\n",
    "    for line in tqdm(f):\n",
    "        key, vector, score = line.strip().split(\"\\t\")\n",
    "        swissprot_keys.append(key)\n",
    "        swissprot_embeddings.append(list(map(float, vector.split())))\n",
    "\n",
    "pdb_embeddings = np.array(pdb_embeddings)\n",
    "swissprot_embeddings = np.array(swissprot_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdb_key_to_index = dict(zip(pdb_keys, range(len(pdb_keys))))\n",
    "swissprot_key_to_index = dict(zip(swissprot_keys, range(len(swissprot_keys))))\n",
    "swissprot_keys_set = set(swissprot_keys)\n",
    "pdb_keys_set = set(pdb_keys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdb_to_uniprots = defaultdict(list)\n",
    "with open(\"data/uniprot_segments_observed.tsv\") as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        pdb_key = f\"{parts[0]}_{parts[1]}\"\n",
    "        swissprot_key = f\"{parts[2]}-F1\"\n",
    "        if pdb_key not in pdb_keys_set or swissprot_key not in swissprot_keys_set:\n",
    "            continue\n",
    "        uniprot_start, uniprot_end = map(int, parts[-2:])\n",
    "        pdb_start, pdb_end = map(int, parts[3:5])\n",
    "        swissprot_length = swissprot_lengths[swissprot_key]\n",
    "        pdb_length = pdb_lengths[pdb_key]\n",
    "        if np.abs(swissprot_length - pdb_length) < 10:\n",
    "            pdb_to_uniprots[pdb_key].append(f\"{parts[2]}-F1\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "indices_same = set()\n",
    "indices_different = []\n",
    "for k in tqdm(pdb_to_uniprots):\n",
    "    for s in pdb_to_uniprots[k]:\n",
    "        indices_same.add((pdb_key_to_index[k], swissprot_key_to_index[s]))\n",
    "indices_same = np.array(list(indices_same))\n",
    "for k, _ in indices_same:\n",
    "    indices_different.append((k, np.random.randint(len(swissprot_keys))))\n",
    "indices_different = np.array(indices_different)\n",
    "indices_same.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distances_same = paired_distances(pdb_embeddings[indices_same[:, 0]],\n",
    "                                  swissprot_embeddings[indices_same[:, 1]],\n",
    "                                  metric=\"euclidean\")\n",
    "distances_different = paired_distances(pdb_embeddings[indices_different[:, 0]],\n",
    "                                       swissprot_embeddings[indices_different[:, 1]],\n",
    "                                       metric=\"euclidean\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.15\n",
    "with plt.style.context('ipynb'):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.hist(distances_different, bins=70, alpha=0.5, label=\"PDB chain and random AF structure\")\n",
    "    plt.hist(distances_same, bins=70, alpha=0.5, label=\"PDB chain and matching AF structure\")\n",
    "    plt.vlines(threshold, 0, 40000, color=\"black\", label=\"distance=0.15\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"data/figures/embedding_distance.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(np.where(distances_same < threshold)[0].shape[0] / len(distances_same),\n",
    " np.where(distances_different < threshold)[0].shape[0] / len(distances_different))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic modelling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(pdb_topic_file, \"rb\") as f:\n",
    "    (pdb_topic_keys, pdb_tfidf_matrix, pdb_w_matrix) = pickle.load(f)\n",
    "\n",
    "with open(uniref_topic_file, \"rb\") as f:\n",
    "    (uniref50_topic_keys, uniref50_tfidf_matrix, uniref50_w_matrix) = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(paper_dir / \"topics_to_proteins.pkl\", \"rb\") as f:\n",
    "    topics_to_proteins = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key_to_topics = defaultdict(list)\n",
    "for t in tqdm(topics_to_proteins):\n",
    "    for key, score in topics_to_proteins[t]:\n",
    "        key_to_topics[key].append((t, score))\n",
    "for k in tqdm(key_to_topics):\n",
    "    key_to_topics[k] = sorted(key_to_topics[k],\n",
    "                              key=lambda x: x[1], reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_uniref = sum(1 for k in wv_keys if \"-\" in k)\n",
    "num_af = sum(uniref50_id_to_num[k] for k in wv_keys if \"-\" in k)\n",
    "num_pdb_chains = sum(1 for k in wv_keys if \"-\" not in k)\n",
    "num_pdb = len(set(k.split(\"_\")[0] for k in wv_keys if \"-\" not in k))\n",
    "num_uniref, num_af, num_pdb_chains, num_pdb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topics = list(range(uniref50_w_matrix.shape[1]))\n",
    "uniref_freqs = np.array([sum(1 for key, _ in topics_to_proteins[i] if \"-\" in key) / num_uniref for i in topics])\n",
    "af_freqs = np.array([sum(uniref50_id_to_num.get(key, 0) for key, _ in topics_to_proteins[i]) / num_af for i in topics])\n",
    "pdb_chain_freqs = np.array(\n",
    "    [sum(1 for key, _ in topics_to_proteins[i] if \"-\" not in key) / num_pdb_chains for i in topics])\n",
    "pdb_freqs = np.array(\n",
    "    [len(set(key.split(\"_\")[0] for key, _ in topics_to_proteins[i] if not \"-\" in key)) / num_pdb for i in topics])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diff = np.abs(af_freqs - pdb_chain_freqs)\n",
    "sort_idx = np.argsort(diff)[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_topics = [topics[i] for i in sort_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_keys_per_topic = {}\n",
    "for i in range(3):\n",
    "    topic = sorted_topics[i]\n",
    "    protein_scores = sorted(topics_to_proteins[topic], key=lambda x: x[1], reverse=True)[:50]\n",
    "    if not len(protein_scores):\n",
    "        continue\n",
    "    indices = np.linspace(0, len(protein_scores) - 1, 4, dtype=int)\n",
    "    example_keys_per_topic[topic] = [protein_scores[x][0] for x in indices]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_shapemer_indices(query_keys):\n",
    "    query_keys_values = set([q for q in query_keys])\n",
    "    query_keys_indices = set([q for q in query_keys])\n",
    "    per_key_values = {}\n",
    "    per_key_indices = {}\n",
    "    with open(uniref_s_file) as f:\n",
    "        for line in f:\n",
    "            key, shapemers = line.strip().split(\"\\t\")\n",
    "            if key not in query_keys_values:\n",
    "                continue\n",
    "            per_key_values[key] = list(map(int, shapemers.split()))\n",
    "            query_keys_values.remove(key)\n",
    "            if not len(query_keys_values):\n",
    "                break\n",
    "    with open(uniref_i_file) as f:\n",
    "        for line in f:\n",
    "            key, indices = line.strip().split(\"\\t\")\n",
    "            if key not in query_keys_indices:\n",
    "                continue\n",
    "            per_key_indices[key] = list(map(int, indices.split()))\n",
    "            query_keys_indices.remove(key)\n",
    "            if not len(query_keys_indices):\n",
    "                break\n",
    "    if len(query_keys_indices):\n",
    "        with open(pdb_s_file) as f:\n",
    "            for line in f:\n",
    "                key, shapemers = line.strip().split(\"\\t\")\n",
    "                if key not in query_keys_values:\n",
    "                    continue\n",
    "                per_key_values[key] = list(map(int, shapemers.split()))\n",
    "                query_keys_values.remove(key)\n",
    "                if not len(query_keys_values):\n",
    "                    break\n",
    "        with open(pdb_i_file) as f:\n",
    "            for line in f:\n",
    "                key, indices = line.strip().split(\"\\t\")\n",
    "                if key not in query_keys_indices:\n",
    "                    continue\n",
    "                per_key_indices[key] = list(map(int, indices.split()))\n",
    "                query_keys_indices.remove(key)\n",
    "                if not len(query_keys_indices):\n",
    "                    break\n",
    "    return {k: dict(zip(per_key_indices[k], per_key_values[k])) for k in query_keys if k in per_key_indices}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shapemer_to_index = {int(k): v for k, v in vectorizer.vocabulary_.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_shapemer_topic_scores(query_keys, topic):\n",
    "    shapemer_indices = get_shapemer_indices(query_keys)\n",
    "    return {k: {i: topic_model.components_[topic][shapemer_to_index[s]] for i, s in shapemer_indices[k].items() if\n",
    "                s in shapemer_to_index} for k in query_keys}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shapemer_topic_scores = {}\n",
    "for t in tqdm(example_keys_per_topic):\n",
    "    shapemer_topic_scores[t] = get_shapemer_topic_scores(example_keys_per_topic[t], t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_dir = Path(paper_dir / \"topic_proteins\")\n",
    "topic_dir.mkdir(exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for t in example_keys_per_topic:\n",
    "    directory = topic_dir / f\"{t}\"\n",
    "    directory.mkdir(exist_ok=True)\n",
    "    for k in shapemer_topic_scores[t]:\n",
    "        if \"-\" in k:\n",
    "            key = k.split(\"-\")[0]\n",
    "            protein = pd.parseMMCIF(f\"data/proteins/{key}-AF-v3.cif\")\n",
    "        else:\n",
    "            key, chain = k.split(\"_\")\n",
    "            protein = pd.parseMMCIF(f\"data/proteins/{key}.cif\", chain=chain)\n",
    "        protein = plotting.get_topic_scores(protein,\n",
    "                                            shapemer_topic_scores[t][k])\n",
    "        pd.writePDB(str(directory / f\"{k}.pdb\"), protein)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_index = pynndescent.NNDescent(wv_embeddings,\n",
    "                                 n_jobs=100,\n",
    "                                 verbose=True,\n",
    "                                 low_memory=True)\n",
    "with open(paper_dir / \"pdb_uniref50_word2vec_embeddings_nn_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nn_index, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neighbor_indices, neighbor_distances = nn_index.neighbor_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key_to_index = dict(zip(wv_keys, range(len(wv_keys))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def annotate_graph(graph, node_keys):\n",
    "    for n in tqdm(node_keys):\n",
    "        k = wv_keys[n]\n",
    "        graph.nodes[k][\"darkness\"] = float(uniref50_id_to_darkness.get(k, 100))\n",
    "        graph.nodes[k][\"outlier_score\"] = float(wv_scores[k])\n",
    "        graph.nodes[k][\"isdark\"] = int(uniref50_id_to_darkness.get(k, 100) <= 5)\n",
    "        graph.nodes[k][\"isbright\"] = int(uniref50_id_to_darkness.get(k, 100) >= 99)\n",
    "        graph.nodes[k][\"isoutlier\"] = int(wv_scores[k] < 0)\n",
    "        graph.nodes[k][\"ispdb\"] = int(\"-\" not in k)\n",
    "        # graph.nodes[k][\"kingdom\"] = get_kingdom(k, data)\n",
    "        # graph.nodes[k][\"interpro\"] = get_interpro(k, data)\n",
    "        # graph.nodes[k][\"IDP\"] = float(get_idp(k, data))\n",
    "        # graph.nodes[k][\"CC\"] = float(get_cc(k, data))\n",
    "        # graph.nodes[k][\"hastm\"] = int(is_tm(k, data))\n",
    "        # graph.nodes[k][\"length\"] = int(get_length(k, data))\n",
    "    return graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Most populated UniRef50 clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_cluster = defaultdict(list)\n",
    "with open(\"AFDBv3_UniRef50_top_most_populated_clusters.csv\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        parts = line.strip().split(\",\")\n",
    "        if float(parts[0]) >= 95:\n",
    "            per_cluster[parts[-1]].append(f\"{parts[1]}-F1\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clusters = list(per_cluster.keys())\n",
    "cluster_counts = np.array([len(per_cluster[c]) for c in clusters])\n",
    "cluster_indices = np.argsort(cluster_counts)[::-1]\n",
    "\n",
    "key_to_cluster = {}\n",
    "for c in cluster_indices:\n",
    "    for k in per_cluster[clusters[c]]:\n",
    "        key_to_cluster[k] = clusters[c]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_cluster_word2vec = defaultdict(list)\n",
    "per_cluster_keys = defaultdict(list)\n",
    "with open(paper_dir / 'uniref50_10_top_word2vec.txt') as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        key, vector = line.strip().split(\"\\t\")\n",
    "        if key not in key_to_cluster:\n",
    "            continue\n",
    "        per_cluster_keys[key_to_cluster[key]].append(key)\n",
    "        per_cluster_word2vec[key_to_cluster[key]].append(list(map(float, vector.split())))\n",
    "for c in tqdm(per_cluster_word2vec):\n",
    "    per_cluster_word2vec[c] = np.array(per_cluster_word2vec[c])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_components(keys, matrix):\n",
    "    u_nn_index = pynndescent.NNDescent(matrix,\n",
    "                                       n_jobs=100,\n",
    "                                       verbose=False,\n",
    "                                       low_memory=True)\n",
    "    u_neighbor_indices, u_neighbor_distances = u_nn_index.neighbor_graph\n",
    "    num = 30\n",
    "    u_graph = nx.Graph()\n",
    "    u_graph.add_nodes_from(keys)\n",
    "    for i, key in enumerate(keys):\n",
    "        for j, distance in zip(u_neighbor_indices[i][:num],\n",
    "                               u_neighbor_distances[i][:num]):\n",
    "            if j == i or distance >= 0.1:\n",
    "                continue\n",
    "            u_graph.add_edge(key, keys[j])\n",
    "    return u_graph, list(nx.connected_components(u_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_to_length = {}\n",
    "with open(paper_dir / 'uniref50_10_top_indices.txt') as f:\n",
    "    for line in tqdm(f):\n",
    "        key, vector = line.strip().split(\"\\t\")\n",
    "        id_to_length[key] = max(map(int, vector.split())) + 16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for index in cluster_indices:\n",
    "    cluster = clusters[index]\n",
    "    length_range = np.array([id_to_length[c] for c in per_cluster_keys[cluster]])\n",
    "    max_length = length_range.max()\n",
    "    keep_indices = [i for i in range(len(per_cluster_keys[cluster])) if length_range[i] > max(100,\n",
    "                                                                                              max_length - 200)]\n",
    "    if not len(keep_indices):\n",
    "        continue\n",
    "    u_graph, u_c = get_components([per_cluster_keys[cluster][i] for i in keep_indices],\n",
    "                                  per_cluster_word2vec[cluster][keep_indices])\n",
    "    if len(u_c) > 1:\n",
    "        max_n = max(len(x) for x in u_c)\n",
    "        print(cluster, len(u_c), f\"{100 * max_n / len(u_graph):.2f}\")\n",
    "        for x in u_c:\n",
    "            print(len(x), list(x)[0],\n",
    "                  f\"{np.mean([id_to_length[k] for k in x]):.2f}\",\n",
    "                  f\"{np.std([id_to_length[k] for k in x]):.2f}\")\n",
    "        print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Structural outliers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num = 4\n",
    "outlier_edge_keys = set()\n",
    "outlier_node_keys = set()\n",
    "outlier_keys = []\n",
    "for i, key in tqdm(enumerate(wv_keys)):\n",
    "    if key in repeat_protein_counts[370]:\n",
    "        continue\n",
    "    if wv_scores[key] < 0:\n",
    "        outlier_keys.append(key)\n",
    "        for j, distance in zip(neighbor_indices[i][:num],\n",
    "                               neighbor_distances[i][:num]):\n",
    "            if j == i or distance >= 0.15 or wv_keys[j] in repeat_protein_counts[370]:\n",
    "                continue\n",
    "            outlier_edge_keys.add((i, j, distance))\n",
    "            outlier_node_keys.add(i)\n",
    "            outlier_node_keys.add(j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outlier_graph = nx.Graph()\n",
    "outlier_graph.add_nodes_from([wv_keys[n] for n in outlier_node_keys])\n",
    "outlier_graph.add_edges_from([(wv_keys[i], wv_keys[j], dict(weight=d)) for i, j, d in outlier_edge_keys])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outlier_graph = annotate_graph(outlier_graph, outlier_node_keys)\n",
    "centralities = nx.degree_centrality(outlier_graph)\n",
    "outlier_components = list(nx.connected_components(outlier_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "representatives = []\n",
    "for c in outlier_components:\n",
    "    key, centrality, len_c = sorted([(k, centralities[k], len(c)) for k in c], key=lambda x: x[1], reverse=True)[0]\n",
    "    num_pdb = sum(1 for k in c if \"-\" not in k)\n",
    "    representatives.append((key, len_c, num_pdb, uniref50_id_to_darkness.get(key, 100)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose examples from representatives"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dark proteins"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num = 4\n",
    "edge_keys = set()\n",
    "node_keys = set()\n",
    "dark_keys = []\n",
    "for i, key in tqdm(enumerate(wv_keys)):\n",
    "    if key in repeat_protein_counts[370]:\n",
    "        continue\n",
    "    if uniref50_id_to_darkness.get(key, 100) <= 5:\n",
    "        dark_keys.append(key)\n",
    "        for j, distance in zip(neighbor_indices[i][:num],\n",
    "                               neighbor_distances[i][:num]):\n",
    "            if j == i or distance >= 0.15 or wv_keys[j] in repeat_protein_counts[370]:\n",
    "                continue\n",
    "            edge_keys.add((i, j, distance))\n",
    "            node_keys.add(i)\n",
    "            node_keys.add(j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "graph.add_nodes_from([wv_keys[n] for n in node_keys])\n",
    "graph.add_edges_from([(wv_keys[i], wv_keys[j]) for i, j, d in edge_keys])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = annotate_graph(graph, node_keys)\n",
    "components = [graph.subgraph(c).copy() for c in nx.connected_components(graph)]\n",
    "component_indices = np.argsort([len(c) for c in components])[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subgraph = components[component_indices[0]]\n",
    "for n in component_indices[1:]:\n",
    "    if len(components[n]) > 100:\n",
    "        subgraph = nx.compose(subgraph, components[n])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nx.write_gml(subgraph, \"word2vec_dark_graph.gml\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}